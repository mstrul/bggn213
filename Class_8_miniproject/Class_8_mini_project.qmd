---
title: "Mini Project"
author: "Max Strul"
format: pdf
editor: visual
date: 10/21/22
toc: TRUE
---

## Mini Project

# Unsupervised learning analysis of breast cancer cells

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```

```{r}
wisc.data <- wisc.df[,-1]
#no more diagnosis column
wisc.data <- wisc.data[,-31]
#got rid of white space
head(wisc.data)
nrow(wisc.data)
ncol(wisc.data)
```

We can use diagnosis as a factor to sum over specific factors

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
```

How many individuals have a diagnosed cancer?

```{r}
table(wisc.df$diagnosis)
```
How many variables have a suffix of "mean"

The `grep()` function will be used here

```{r}
#obtain column names:
col.names.vector <- colnames(wisc.data)
#this is a vector
length(grep("_mean",col.names.vector))

```
# Starting the principal component analysis

Lets try PCA on this data to see what major features might be hidden in this large dimensional dataset. 

Functions we want to use:
`prcomp()`

`hclust(dist(x))`

`cutree(x,k=?)`

Initial data analysis **scaling** 

```{r}
round(colMeans(wisc.data), 2)

round(apply(wisc.data, 2, sd), 2)
```
```{r}
scale_false <- prcomp(wisc.data, scale = FALSE)
scale_true <- prcomp(wisc.data, scale = TRUE)
summary(scale_false)
summary(scale_true)
#We will continue with scale = true

wisc.pr <- prcomp(wisc.data,scale. = TRUE)
```
An example of when you don't want to re-scale is when you have all of the same units data (example a .pdb structure file, all the data is the same between proteins, and if you re-scale then you are altering the data!) 

We see here that if we are un-scaled then PC1 covers 98 percent of the variance, but when we scale its only 44%

```{r}
#we can perform a score plot 
#aka a P.C. plot or
#ordination plots 

head(wisc.pr$x)
head(wisc.pr$rotation)

plot(wisc.pr$x[,1],wisc.pr$x[,2])
```
** each dot here is a patient! ** 

We can now color based on diagnosis 

```{r}

plot(wisc.pr$x[,1],wisc.pr$x[,2],col=diagnosis,title="Malginant (red) vs Non Malignant (Black)")

```
Can we plot other PCAs to see if we see such a stark contrast?

```{r}
plot(wisc.pr$x[,2],wisc.pr$x[,3],col=diagnosis)

```

```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,3],col=diagnosis)
```

```{r}
plot(wisc.pr$x[,3],wisc.pr$x[,4],col=diagnosis)
```

Comparing PCA1 and PCA2 are the best. 1 and 3 also work. 

So now we want to eventually use these graphs and auto generate this so that we can find out which gives us the greatest distance between the two groups

we can use the distance function to do so?

```{r}

wisc.pr.pca.1.2 <- cbind(x=wisc.pr$x[,1],y=wisc.pr$x[,2])

wisc.pr.tree <- hclust(dist(wisc.pr.pca.1.2))

wisc.pr.tree.original <- hclust(dist(wisc.data))

plot(wisc.pr.tree.original)

plot(wisc.pr.tree)

#it looks like height of 22 looks like a good point to cut
abline(h=22)

tree_groups <- cutree(wisc.pr.tree.original,k=2) 

#Here we see that of the two groups,
#the group separation was very poor.

table(diagnosis, tree_groups)

plot(wisc.pr$x[,1],wisc.pr$x[,2],col=tree_groups)

#k=2 here becauase we want two groups
#km.pr <- kmeans(wisc.pr.pca.1.2, centers = 2, nstart = 20)

#plot(wisc.pr.pca.1.2, col=km.pr$cluster)
```
```{r}

newtree.pca <- hclust(dist(wisc.pr$x[,1:2]),method="ward.D2")
plot(newtree.pca)
table(diagnosis, cutree(newtree.pca,k=2))
```
Here we see that the three PCAs allow us to develop a table much better!

Infact the sensitivtiy of this algorithm might be good, lets find out!

```{r}
summary(diagnosis)
```

```{r}
tn <- 339
tp <- 177
fn <- 18
fp <- 35
sensitivity <- tp/(tp+fn)
specifcity <- tn/(fp+tn)
#now we want to find out the values:

sensitivity
specifcity

```
You can explore through utilizing more of the PCAs, however the sensitivty and specificty get worse! For example:

sensitivity and specificity for using [,1:3]:

sens:
0.88

spec:
0.91

sensitivity and specificity for using [,1:2]:

sens:
0.91

spec:
0.91

So actually, using just the first two PCAs provides a better model!

# We can now use this as a predictive method!

`predict()` will take our PCA model from before and you can add new data to project onto our PCA space

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata = new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col = diagnosis)
points(npc[,1],npc[,2],col="blue", pch=16, cex =3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
I would follow up with patient 2, since the red group is the malignant group and patient 1 is closer to the non malignant group. 

```{r}
sessionInfo()
```


Remember our goal is to:
1.) Reduce dimensionality
2.) Visualize multidimensional data
3.) Choose the most useful variables (features or PCA0s
4.) Identify groupings of objects 
5.) Identify outliars and remove if necessary




